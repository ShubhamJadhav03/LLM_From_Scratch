â­ 14-Day LLM Bootcamp â€” LinkedIn Update Template

Below is a ready-to-use template for each day, matching your exact timeline.

ğŸ“Œ Day 1 â€” Data Preparation (Videos 1â€“5)

Todayâ€™s Focus:
Tokenization, sliding windows, and building dataloaders.

What I did:

Preprocessed raw text

Converted text â†’ token IDs

Created input/target pairs using PyTorch DataLoader

Win: Finally seeing batches of token IDs come out cleanly.
Struggle: Off-by-one sequence alignmentâ€¦ my new enemy.

Tomorrow: Embeddings + positional encoding.
Letâ€™s go. ğŸš€

ğŸ“Œ Day 2 â€” Embeddings & Positional Encoding (Videos 6â€“9)

Todayâ€™s Focus:
Mapping token IDs to vectors + encoding order.

What I did:

Implemented token + positional embeddings

Understood why order changes meaning

Visualized embedding shapes

Win: Everything runs without shape mismatches.
Struggle: Getting sinusoidal PE right.

Next: Self-attention â€” the beast begins tomorrow.

ğŸ“Œ Day 3 â€” Attention Mechanism Part 1 (Videos 10â€“13)

Todayâ€™s Focus:
Self-Attention, Q/K/V, and attention scores.

What I did:

Implemented Query, Key, Value projections

Calculated attention weights

Understood why attention â€œfocusesâ€

Win: Attention matrix working!
Struggle: Normalizing attention correctly.

Next: Multi-head attention + masking.

ğŸ“Œ Day 4 â€” Attention Mechanism Part 2 (Videos 14â€“17)

Todayâ€™s Focus:
Causal masking + Multi-Head Attention.

What I did:

Implemented causal mask (no peeking at future tokens)

Built MultiHeadAttention class

Verified output shapes

Win: Multi-head attention doesnâ€™t explode.
Struggle: That masking logic is brutal.

Next: Transformer block assembly.

ğŸ“Œ Day 5 â€” Building the Transformer Block (Videos 18â€“21)

Todayâ€™s Focus:
LayerNorm + FeedForward + residuals.

What I did:

Implemented TransformerBlock

Added GELU + linear layers

Connected attention + FFN with skip connections

Win: Everything runs end-to-end.
Struggle: Keeping track of so many tensors.

Next: Build the full GPT.

ğŸ“Œ Day 6 â€” The Full GPT Model (Videos 22â€“25)

Todayâ€™s Focus:
Assembling the full model.

What I did:

Combined blocks â†’ GPTModel

Added final layer

Generated first â€œrandomâ€ outputs

Win: Model runs without crashing!
Struggle: Debugged residual paths for hours.

Next: Debug buffer day.

ğŸ“Œ Day 7 â€” Buffer / Debugging Day

Todayâ€™s Focus:
Fix shapes, test everything, sanity-check math.

What I did:

Cleaned up functions

Added assertions + tests

Refactored critical sections

Win: Clean architecture, clean mind.
Struggle: Tensor shapesâ€¦ always tensor shapes.

Next: Start training!

ğŸ“Œ Day 8 â€” Pretraining Loop (Videos 26â€“29)

Todayâ€™s Focus:
Training step + loss function + backprop.

What I did:

Wrote training function

Implemented CrossEntropy loss

Ran first forward/backward pass

Win: Loss decreases!
Struggle: GPU memory tantrums.

Next: Text generation sampling.

ğŸ“Œ Day 9 â€” Text Generation Strategies (Videos 30â€“32)

Todayâ€™s Focus:
Temperature, top-k, top-p sampling.

What I did:

Wrote sampling function

Compared strategies

Generated first coherent-ish text

Win: Model actually produces words!
Struggle: Sampling hyperparameters.

Next: Load GPT-2 weights.

ğŸ“Œ Day 10 â€” Loading Pretrained Weights (Videos 33â€“35)

Todayâ€™s Focus:
Bring the GPT-2 brain into my GPT architecture.

What I did:

Mapped imported weights

Validated layer-by-layer shapes

Generated first GPT-2â€“level output

Win: Huge jump in quality.
Struggle: Matching exact layer names.

Next: Fine-tuning classification.

ğŸ“Œ Day 11 â€” Fine-Tuning for Classification (Videos 36â€“38)

Todayâ€™s Focus:
Spam detection / sentiment classifier.

What I did:

Added classification head

Trained on labeled dataset

Evaluated accuracy + metrics

Win: Fine-tuned model works!
Struggle: Dataset cleaning.

Next: Prepare instruction-tuning dataset.

ğŸ“Œ Day 12 â€” Instruction Fine-Tuning Part 1 (Videos 39â€“40)

Todayâ€™s Focus:
Build instruction-following dataset.

What I did:

Built Alpaca-style JSON dataset

Added promptâ†’response formatting

Validated sample entries

Win: Dataset is clean and usable.
Struggle: Formatting consistency.

Next: Final training.

ğŸ“Œ Day 13 â€” Instruction Fine-Tuning Part 2 (Videos 41â€“43)

Todayâ€™s Focus:
Final instruction tuning run.

What I did:

Ran training

Evaluated on prompts

Fixed alignment issues

Win: Model follows instructions!
Struggle: Training stability.

Next: Final polish & wrap-up.

ğŸ“Œ Day 14 â€” Final Polish + Documentation

Todayâ€™s Focus:
Wrap up the whole project.

What I did:

Cleaned scripts

Created a simple inference interface

Wrote summary + architecture diagram

Win: End-to-end LLM built from scratch.
Struggle: None â€” final day celebrations ğŸ˜„

Next: Share learnings + final demo.