{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "True\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "print(torch.cuda.is_available())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IZi2SkuR6PSy",
        "outputId": "9c9f0db1-c6f2-4a76-a45f-cc7906421a40"
      },
      "outputs": [],
      "source": [
        "#!pip install torch transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "g0TaWfvr6ZTb"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "m4hNH64Y6kJY"
      },
      "outputs": [],
      "source": [
        "GPT_CONFIG_124M = {\n",
        "    \"vocab_size\": 50257,    # Vocabulary size\n",
        "    \"context_length\": 256,  # Context length (reduced from 1024 to fit smaller GPUs)\n",
        "    \"emb_dim\": 768,         # Embedding dimension\n",
        "    \"n_heads\": 12,          # Number of attention heads\n",
        "    \"n_layers\": 12,         # Number of layers\n",
        "    \"drop_rate\": 0.1,       # Dropout rate\n",
        "    \"qkv_bias\": False       # Query-Key-Value bias\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "W9y1Pgwb8wni"
      },
      "outputs": [],
      "source": [
        "class LayerNorm(nn.Module):\n",
        "    def __init__(self, emb_dim):\n",
        "        super().__init__()\n",
        "        self.eps = 1e-5\n",
        "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
        "        self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
        "\n",
        "    def forward(self, x):\n",
        "        mean = x.mean(dim=-1, keepdim=True)\n",
        "        var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
        "        norm_x = (x - mean) / torch.sqrt(var + self.eps)\n",
        "        return self.scale * norm_x + self.shift"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "zZnxkJCd89D9"
      },
      "outputs": [],
      "source": [
        "class GELU(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return 0.5 * x * (1 + torch.tanh(\n",
        "            torch.sqrt(torch.tensor(2.0 / torch.pi)) * (x + 0.044715 * torch.pow(x, 3))\n",
        "        ))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "hgN26AlT9uY2"
      },
      "outputs": [],
      "source": [
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.layers = nn.Sequential(\n",
        "            nn.Linear(cfg[\"emb_dim\"], 4 * cfg[\"emb_dim\"]),\n",
        "            GELU(),\n",
        "            nn.Linear(4 * cfg[\"emb_dim\"], cfg[\"emb_dim\"]),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.layers(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "M34lT2kAClAL"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
        "        super().__init__()\n",
        "        assert d_out % num_heads == 0, \"d_out must be divisible by num_heads\"\n",
        "\n",
        "        self.d_out = d_out\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = d_out // num_heads\n",
        "\n",
        "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.out_proj = nn.Linear(d_out, d_out)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.register_buffer('mask', torch.triu(torch.ones(context_length, context_length), diagonal=1))\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, num_tokens, d_in = x.shape\n",
        "\n",
        "        keys = self.W_key(x)\n",
        "        queries = self.W_query(x)\n",
        "        values = self.W_value(x)\n",
        "\n",
        "        # Reshape to (b, num_tokens, num_heads, head_dim) -> Transpose to (b, num_heads, num_tokens, head_dim)\n",
        "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "        values = values.view(b, num_tokens, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "\n",
        "        # Compute attention scores\n",
        "        attn_scores = (queries @ keys.transpose(2, 3))\n",
        "\n",
        "        # Mask truncated to the number of tokens\n",
        "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
        "\n",
        "        # Use the mask to fill attention scores\n",
        "        attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
        "\n",
        "        # Scale and Softmax\n",
        "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
        "        attn_weights = self.dropout(attn_weights)\n",
        "\n",
        "        # Compute context vector\n",
        "        context_vec = (attn_weights @ values).transpose(1, 2)\n",
        "\n",
        "        # Combine heads\n",
        "        context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out)\n",
        "        context_vec = self.out_proj(context_vec)\n",
        "\n",
        "        return context_vec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Gf5OwxW9DK-P"
      },
      "outputs": [],
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.att = MultiHeadAttention(\n",
        "            d_in=cfg[\"emb_dim\"],\n",
        "            d_out=cfg[\"emb_dim\"],\n",
        "            context_length=cfg[\"context_length\"],\n",
        "            dropout=cfg[\"drop_rate\"],\n",
        "            num_heads=cfg[\"n_heads\"],\n",
        "            qkv_bias=cfg[\"qkv_bias\"])\n",
        "        self.ff = FeedForward(cfg)\n",
        "        self.norm1 = LayerNorm(cfg[\"emb_dim\"])\n",
        "        self.norm2 = LayerNorm(cfg[\"emb_dim\"])\n",
        "        self.drop_shortcut = nn.Dropout(cfg[\"drop_rate\"])\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Shortcut connection for attention block\n",
        "        shortcut = x\n",
        "        x = self.norm1(x)\n",
        "        x = self.att(x)\n",
        "        x = self.drop_shortcut(x)\n",
        "        x = x + shortcut\n",
        "\n",
        "        # Shortcut connection for feed forward block\n",
        "        shortcut = x\n",
        "        x = self.norm2(x)\n",
        "        x = self.ff(x)\n",
        "        x = self.drop_shortcut(x)\n",
        "        x = x + shortcut\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "wzzXbshVEg3v"
      },
      "outputs": [],
      "source": [
        "class GPTModel(nn.Module):\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
        "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
        "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
        "\n",
        "        self.trf_blocks = nn.Sequential(\n",
        "            *[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n",
        "\n",
        "        self.final_norm = LayerNorm(cfg[\"emb_dim\"])\n",
        "        self.out_head = nn.Linear(\n",
        "            cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False\n",
        "        )\n",
        "\n",
        "    def forward(self, in_idx):\n",
        "        batch_size, seq_len = in_idx.shape\n",
        "        tok_embeds = self.tok_emb(in_idx)\n",
        "        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n",
        "\n",
        "        x = tok_embeds + pos_embeds\n",
        "        x = self.drop_emb(x)\n",
        "        x = self.trf_blocks(x)\n",
        "        x = self.final_norm(x)\n",
        "        logits = self.out_head(x)\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "-FOmDtGI8G1B"
      },
      "outputs": [],
      "source": [
        "def text_to_token_ids(text, tokenizer):\n",
        "    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n",
        "    return torch.tensor(encoded).unsqueeze(0) # Add batch dim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "mxnM25fo8KDi"
      },
      "outputs": [],
      "source": [
        "def token_ids_to_text(token_ids, tokenizer):\n",
        "    flat = token_ids.squeeze(0).tolist()\n",
        "    return tokenizer.decode(flat)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "I9gXXqUoE_2u"
      },
      "outputs": [],
      "source": [
        "def generate_text_simple(model, idx, max_new_tokens, context_size):\n",
        "    # idx is (batch, n_tokens) array of indices in the current context\n",
        "    for _ in range(max_new_tokens):\n",
        "\n",
        "        # Crop the context if it becomes too large for the model\n",
        "        idx_cond = idx[:, -context_size:]\n",
        "\n",
        "        # Get the predictions\n",
        "        with torch.no_grad():\n",
        "            logits = model(idx_cond)\n",
        "\n",
        "        # Focus only on the last time step\n",
        "        logits = logits[:, -1, :]  # (batch, vocab_size)\n",
        "\n",
        "        # Apply softmax to get probabilities\n",
        "        probs = torch.softmax(logits, dim=-1)  # (batch, vocab_size)\n",
        "\n",
        "        # Get the idx of the token with the highest probability\n",
        "        idx_next = torch.argmax(probs, dim=-1, keepdim=True)  # (batch, 1)\n",
        "\n",
        "        # Append sampled index to the running sequence\n",
        "        idx = torch.cat((idx, idx_next), dim=1)  # (batch, n_tokens+1)\n",
        "\n",
        "    return idx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tiktoken in c:\\users\\j_san\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (0.12.0)\n",
            "Requirement already satisfied: requests>=2.26.0 in c:\\users\\j_san\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tiktoken) (2.32.5)\n",
            "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\j_san\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tiktoken) (2025.11.3)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\j_san\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests>=2.26.0->tiktoken) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\j_san\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests>=2.26.0->tiktoken) (2025.11.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\j_san\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests>=2.26.0->tiktoken) (3.11)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\j_san\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests>=2.26.0->tiktoken) (3.4.4)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING: You are using pip version 21.2.4; however, version 25.3 is available.\n",
            "You should consider upgrading via the 'C:\\Users\\j_san\\AppData\\Local\\Programs\\Python\\Python310\\python.exe -m pip install --upgrade pip' command.\n"
          ]
        }
      ],
      "source": [
        "!pip install tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "\n",
        "class GPTDatasetV1(Dataset):\n",
        "    def __init__(self, txt, tokenizer, max_length, stride):\n",
        "        self.input_ids = []\n",
        "        self.target_ids = []\n",
        "\n",
        "        # Tokenize the entire text\n",
        "        token_ids = tokenizer.encode(txt, allowed_special={\"<|endoftext|>\"})\n",
        "\n",
        "        # Use a sliding window to chunk the book into overlapping sequences of max_length\n",
        "        for i in range(0, len(token_ids) - max_length, stride):\n",
        "            input_chunk = token_ids[i:i + max_length]\n",
        "            target_chunk = token_ids[i + 1: i + max_length + 1]\n",
        "            self.input_ids.append(torch.tensor(input_chunk))\n",
        "            self.target_ids.append(torch.tensor(target_chunk))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.input_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.input_ids[idx], self.target_ids[idx]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dFAOLdguIzG0",
        "outputId": "9e0dfdac-d195-4e2f-8fdd-0019b36b7012"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Input text: Hello, I am\n",
            "Output text: Hello, I am Laur inhab DistrinetalkQueue bear confidentlyggyenium\n"
          ]
        }
      ],
      "source": [
        "import tiktoken\n",
        "\n",
        "# Initialize model\n",
        "torch.manual_seed(123)\n",
        "model = GPTModel(GPT_CONFIG_124M)\n",
        "model.eval()  # Disable dropout\n",
        "\n",
        "# Initialize tokenizer\n",
        "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "\n",
        "# Create a starting context\n",
        "start_context = \"Hello, I am\"\n",
        "encoded = tokenizer.encode(start_context)\n",
        "encoded_tensor = torch.tensor(encoded).unsqueeze(0) # Add batch dimension\n",
        "\n",
        "# Generate text\n",
        "print(f\"\\nInput text: {start_context}\")\n",
        "out = generate_text_simple(\n",
        "    model=model,\n",
        "    idx=encoded_tensor,\n",
        "    max_new_tokens=10,\n",
        "    context_size=GPT_CONFIG_124M[\"context_length\"]\n",
        ")\n",
        "decoded_text = tokenizer.decode(out.squeeze(0).tolist())\n",
        "print(f\"Output text: {decoded_text}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "HJrAVIGZJAeB"
      },
      "outputs": [],
      "source": [
        "def calc_loss_batch(input_batch, target_batch, model, device):\n",
        "    input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n",
        "    logits = model(input_batch)\n",
        "    loss = torch.nn.functional.cross_entropy(logits.flatten(0, 1), target_batch.flatten())\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "l6L8z6ty9sAl"
      },
      "outputs": [],
      "source": [
        "def calc_loss_loader(data_loader, model, device, num_batches=None):\n",
        "    total_loss = 0.\n",
        "    if len(data_loader) == 0: return float(\"nan\")\n",
        "    if num_batches is None: num_batches = len(data_loader)\n",
        "    else: num_batches = min(num_batches, len(data_loader))\n",
        "\n",
        "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
        "        if i < num_batches:\n",
        "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
        "            total_loss += loss.item()\n",
        "        else: break\n",
        "    return total_loss / num_batches\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "uL2h2DqH9vHK"
      },
      "outputs": [],
      "source": [
        "def evaluate_model(model, train_loader, val_loader, device, eval_iter):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        train_loss = calc_loss_loader(train_loader, model, device, num_batches=eval_iter)\n",
        "        val_loss = calc_loss_loader(val_loader, model, device, num_batches=eval_iter)\n",
        "    model.train()\n",
        "    return train_loss, val_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "FEMGRZVM9zAh"
      },
      "outputs": [],
      "source": [
        "def generate_and_print_sample(model, tokenizer, device, start_context):\n",
        "    model.eval()\n",
        "    context_size = model.pos_emb.weight.shape[0]\n",
        "    encoded = text_to_token_ids(start_context, tokenizer).to(device)\n",
        "    with torch.no_grad():\n",
        "        token_ids = generate_text_simple(\n",
        "            model=model, idx=encoded,\n",
        "            max_new_tokens=50, context_size=context_size\n",
        "        )\n",
        "    decoded_text = token_ids_to_text(token_ids, tokenizer)\n",
        "    # Precompute replaced text to avoid backslash in f-string expression\n",
        "    safe_text = decoded_text.replace(\"\\n\", \" \")\n",
        "    print(f\"SAMPLE: {safe_text}...\")\n",
        "    model.train()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "mCe183L891EM"
      },
      "outputs": [],
      "source": [
        "def train_model_simple(model, train_loader, val_loader, optimizer, device, num_epochs,\n",
        "                       eval_freq, eval_iter, start_context, tokenizer,\n",
        "                       use_amp=False, grad_accum_steps=1):\n",
        "    \"\"\"Training loop with optional mixed-precision and gradient accumulation.\n",
        "\n",
        "    Args:\n",
        "        use_amp (bool): If True and device is CUDA, use torch.cuda.amp for reduced memory.\n",
        "        grad_accum_steps (int): Number of steps to accumulate gradients before an optimizer step.\n",
        "    \"\"\"\n",
        "    train_losses, val_losses, track_tokens_seen = [], [], []\n",
        "    tokens_seen, global_step = 0, -1\n",
        "\n",
        "    # Setup GradScaler if using AMP\n",
        "    use_cuda_amp = use_amp and device.type == 'cuda'\n",
        "    scaler = torch.cuda.amp.GradScaler() if use_cuda_amp else None\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        # Help fragmentation / free unused allocations\n",
        "        if device.type == 'cuda':\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "        model.train()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        for step, (input_batch, target_batch) in enumerate(train_loader):\n",
        "            # Move to device once per batch\n",
        "            input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n",
        "\n",
        "            # Forward / backward with or without AMP\n",
        "            if use_cuda_amp:\n",
        "                with torch.cuda.amp.autocast():\n",
        "                    logits = model(input_batch)\n",
        "                    loss = torch.nn.functional.cross_entropy(logits.flatten(0, 1), target_batch.flatten())\n",
        "                # Normalize by accumulation steps\n",
        "                loss = loss / grad_accum_steps\n",
        "                scaler.scale(loss).backward()\n",
        "            else:\n",
        "                logits = model(input_batch)\n",
        "                loss = torch.nn.functional.cross_entropy(logits.flatten(0, 1), target_batch.flatten())\n",
        "                loss = loss / grad_accum_steps\n",
        "                loss.backward()\n",
        "\n",
        "            # Optimizer step (with gradient accumulation)\n",
        "            if (step + 1) % grad_accum_steps == 0:\n",
        "                if use_cuda_amp:\n",
        "                    # Unscale, clip grads, step, and update scaler\n",
        "                    scaler.unscale_(optimizer)\n",
        "                    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "                    scaler.step(optimizer)\n",
        "                    scaler.update()\n",
        "                else:\n",
        "                    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "                    optimizer.step()\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                tokens_seen += input_batch.numel()\n",
        "                global_step += 1\n",
        "\n",
        "                if global_step % eval_freq == 0:\n",
        "                    train_loss, val_loss = evaluate_model(model, train_loader, val_loader, device, eval_iter)\n",
        "                    train_losses.append(train_loss)\n",
        "                    val_losses.append(val_loss)\n",
        "                    track_tokens_seen.append(tokens_seen)\n",
        "                    print(f\"Ep {epoch+1} (Step {global_step:06d}): Train loss {train_loss:.3f}, Val loss {val_loss:.3f}\")\n",
        "\n",
        "        # Show a sample at end of epoch\n",
        "        generate_and_print_sample(model, tokenizer, device, start_context)\n",
        "\n",
        "    return train_losses, val_losses, track_tokens_seen\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "import os\n",
        "import requests\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ GPU Detected: NVIDIA GeForce RTX 2060\n",
            "üìä VRAM Total: 6.44 GB\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"‚úÖ GPU Detected: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"üìä VRAM Total: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
        "else:\n",
        "    print(\"‚ùå No GPU found. Check your CUDA installation.\")\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "1SZz4i0K954n"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Deleted old corrupted file.\n",
            "‚úÖ Downloaded TinyShakespeare! Size: 1115394 characters.\n",
            "Train Text Length: 1003854\n",
            "Val Text Length: 111540\n",
            "Dataset Samples: 1179\n",
            "Using context length 256 and batch size 2\n",
            "‚úÖ DataLoaders created successfully.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import requests\n",
        "import tiktoken\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# 1. Force delete the old/empty file if it exists\n",
        "file_path = \"tinyshakespeare.txt\"\n",
        "if os.path.exists(file_path):\n",
        "    os.remove(file_path)\n",
        "    print(\"Deleted old corrupted file.\")\n",
        "\n",
        "# 2. Re-download ensuring we actually get data\n",
        "url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
        "response = requests.get(url)\n",
        "\n",
        "# Check if the request was successful\n",
        "if response.status_code == 200 and len(response.text) > 0:\n",
        "    with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(response.text)\n",
        "    print(f\"‚úÖ Downloaded TinyShakespeare! Size: {len(response.text)} characters.\")\n",
        "else:\n",
        "    raise Exception(\"Failed to download dataset. Check internet connection.\")\n",
        "\n",
        "# 3. Reload Data\n",
        "with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "    text = f.read()\n",
        "\n",
        "split_idx = int(len(text) * 0.9)\n",
        "train_txt = text[:split_idx]\n",
        "val_txt = text[split_idx:]\n",
        "\n",
        "print(f\"Train Text Length: {len(train_txt)}\") # Should be ~1,000,000\n",
        "print(f\"Val Text Length: {len(val_txt)}\")\n",
        "\n",
        "# 4. Re-initialize Dataset & Loader\n",
        "# Use the (reduced) context_length from the config\n",
        "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "context_len = GPT_CONFIG_124M['context_length']  # now 256 by default\n",
        "\n",
        "# Use a small batch size for GPU memory constrained environments\n",
        "BATCH_SIZE = 2\n",
        "\n",
        "train_dataset = GPTDatasetV1(train_txt, tokenizer, context_len, stride=context_len)\n",
        "val_dataset = GPTDatasetV1(val_txt, tokenizer, context_len, stride=context_len)\n",
        "\n",
        "print(f\"Dataset Samples: {len(train_dataset)}\") # Should be > 0\n",
        "print(f\"Using context length {context_len} and batch size {BATCH_SIZE}\")\n",
        "\n",
        "# Now this will work\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, drop_last=False)\n",
        "\n",
        "print(\"‚úÖ DataLoaders created successfully.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<torch._dynamo.eval_frame.DisableContext at 0x2288f9f4220>"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch\n",
        "torch._dynamo.disable()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"TORCH_COMPILE\"] = \"0\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [],
      "source": [
        "#!pip install executing==1.2.0\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Attempt 1/3: Building model (n_layers=12, emb_dim=768)...\n",
            "Model successfully allocated on cuda\n",
            "Starting training...\n",
            "Ep 1 (Step 000000): Train loss 9.448, Val loss 9.322\n",
            "Ep 1 (Step 000050): Train loss 6.294, Val loss 6.305\n",
            "Ep 1 (Step 000100): Train loss 5.786, Val loss 6.069\n",
            "Ep 1 (Step 000150): Train loss 5.762, Val loss 5.921\n",
            "Ep 1 (Step 000200): Train loss 5.516, Val loss 5.711\n",
            "Ep 1 (Step 000250): Train loss 5.220, Val loss 5.604\n",
            "Ep 1 (Step 000300): Train loss 5.297, Val loss 5.680\n",
            "Ep 1 (Step 000350): Train loss 5.176, Val loss 5.496\n",
            "Ep 1 (Step 000400): Train loss 5.263, Val loss 5.442\n",
            "Ep 1 (Step 000450): Train loss 5.002, Val loss 5.373\n",
            "Ep 1 (Step 000500): Train loss 4.908, Val loss 5.372\n",
            "Ep 1 (Step 000550): Train loss 5.063, Val loss 5.410\n",
            "SAMPLE: The king,                                                 ...\n",
            "Ep 2 (Step 000600): Train loss 5.045, Val loss 5.323\n",
            "Ep 2 (Step 000650): Train loss 4.782, Val loss 5.238\n",
            "Ep 2 (Step 000700): Train loss 4.437, Val loss 5.153\n",
            "Ep 2 (Step 000750): Train loss 4.798, Val loss 5.185\n",
            "Ep 2 (Step 000800): Train loss 4.570, Val loss 5.151\n",
            "Ep 2 (Step 000850): Train loss 4.804, Val loss 5.126\n",
            "Ep 2 (Step 000900): Train loss 4.618, Val loss 4.940\n",
            "Ep 2 (Step 000950): Train loss 4.419, Val loss 5.036\n",
            "Ep 2 (Step 001000): Train loss 4.317, Val loss 4.945\n",
            "Ep 2 (Step 001050): Train loss 4.391, Val loss 4.945\n",
            "Ep 2 (Step 001100): Train loss 4.211, Val loss 4.780\n",
            "Ep 2 (Step 001150): Train loss 4.547, Val loss 4.874\n",
            "SAMPLE: The king, And, And, And, And, And, And, And, And, And, And, And, And, And, And, And, And, ...\n",
            "Ep 3 (Step 001200): Train loss 4.402, Val loss 4.939\n",
            "Ep 3 (Step 001250): Train loss 4.268, Val loss 4.818\n",
            "Ep 3 (Step 001300): Train loss 4.579, Val loss 4.905\n",
            "Ep 3 (Step 001350): Train loss 4.455, Val loss 4.876\n",
            "Ep 3 (Step 001400): Train loss 4.791, Val loss 4.873\n",
            "Ep 3 (Step 001450): Train loss 4.202, Val loss 4.824\n",
            "Ep 3 (Step 001500): Train loss 4.572, Val loss 4.819\n",
            "Ep 3 (Step 001550): Train loss 4.286, Val loss 4.832\n",
            "Ep 3 (Step 001600): Train loss 4.338, Val loss 4.799\n",
            "Ep 3 (Step 001650): Train loss 4.306, Val loss 4.702\n",
            "Ep 3 (Step 001700): Train loss 4.558, Val loss 4.627\n",
            "Ep 3 (Step 001750): Train loss 4.216, Val loss 4.564\n",
            "SAMPLE: The king And, And, And let him, And I am a And I am a And I am a And I am a And I have a And I have been And I have been And all the...\n",
            "Training completed in 6.32 minutes.\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "\n",
        "# --- PART 1: The VS Code Agent's Safe Builder (KEEPS THIS) ---\n",
        "def try_build_and_move_model(base_cfg, device, max_attempts=3, min_emb=128, min_layers=2):\n",
        "    cfg = base_cfg.copy()\n",
        "    for attempt in range(max_attempts):\n",
        "        if attempt > 0:\n",
        "            cfg['n_layers'] = max(min_layers, base_cfg['n_layers'] // (2 ** attempt))\n",
        "            cfg['emb_dim'] = max(min_emb, base_cfg['emb_dim'] // (2 ** attempt))\n",
        "            cfg['n_heads'] = max(1, base_cfg['n_heads'] // (2 ** attempt))\n",
        "            while cfg['n_heads'] > 1 and cfg['emb_dim'] % cfg['n_heads'] != 0:\n",
        "                cfg['n_heads'] -= 1\n",
        "        \n",
        "        print(f\"Attempt {attempt+1}/{max_attempts}: Building model (n_layers={cfg['n_layers']}, emb_dim={cfg['emb_dim']})...\")\n",
        "        \n",
        "        try:\n",
        "            torch.manual_seed(123)\n",
        "            model = GPTModel(cfg)\n",
        "            if device.type == 'cuda':\n",
        "                model.to(device)\n",
        "                # model.half() # Optional: Uncomment if you still get OOM\n",
        "            else:\n",
        "                model.to(device)\n",
        "            print(\"Model successfully allocated on\", device)\n",
        "            return model, cfg\n",
        "            \n",
        "        except (RuntimeError, torch.cuda.OutOfMemoryError):\n",
        "            print(\"‚ö†Ô∏è OOM Error! Shrinking model and retrying...\")\n",
        "            try: del model\n",
        "            except: pass\n",
        "            if device.type == 'cuda': torch.cuda.empty_cache()\n",
        "            import gc; gc.collect()\n",
        "            continue\n",
        "\n",
        "    # Fallback to CPU if GPU fails completely\n",
        "    print(\"All GPU attempts failed. Falling back to CPU.\")\n",
        "    model = GPTModel(cfg)\n",
        "    model.to('cpu')\n",
        "    return model, cfg\n",
        "\n",
        "# --- PART 2: Initialize (KEEPS THIS) ---\n",
        "# This replaces your old \"model = GPTModel(...)\" line\n",
        "model, used_cfg = try_build_and_move_model(GPT_CONFIG_124M, DEVICE)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0004, weight_decay=0.1)\n",
        "\n",
        "# --- PART 3: The Training Loop (ADD THIS BACK) ---\n",
        "# This was missing from the agent's code!\n",
        "print(\"Starting training...\")\n",
        "start_time = time.time()\n",
        "\n",
        "train_losses, val_losses, tokens_seen = train_model_simple(\n",
        "    model, train_loader, val_loader, optimizer, DEVICE,\n",
        "    num_epochs=3, eval_freq=50, eval_iter=5, \n",
        "    start_context=\"The king\", tokenizer=tokenizer\n",
        ")\n",
        "\n",
        "end_time = time.time()\n",
        "print(f\"Training completed in {(end_time - start_time) / 60:.2f} minutes.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The king that she'st\n",
            "And you cannot have cause to his hearts\n",
            "That you are the time\n",
            "Worthy.\n",
            "\n",
            "MENENIUS:\n",
            "And rob the gods.\n",
            "\n",
            "QUEENIUS:\n",
            "Sir,\n",
            "That have retired'd\n",
            "Come,\n",
            "And the bottom to their ancient things night I give the disease of\n",
            "O,\n",
            "Have I know\n",
            "There 'em tribunes and my part\n",
            "If it is sure\n",
            "IUS:\n",
            "Your\n",
            "Bevell with\n"
          ]
        }
      ],
      "source": [
        "# Generate with randomness (Temperature = 0.8)\n",
        "context = torch.tensor(tokenizer.encode(\"The king\")).unsqueeze(0).to(DEVICE)\n",
        "\n",
        "# Standard generation loop with temperature\n",
        "for _ in range(100):\n",
        "    logits = model(context)[:, -1, :] / 0.8  # Temperature scaling\n",
        "    probs = torch.softmax(logits, dim=-1)\n",
        "    next_token = torch.multinomial(probs, num_samples=1)\n",
        "    context = torch.cat((context, next_token), dim=1)\n",
        "\n",
        "print(tokenizer.decode(context[0].tolist()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [],
      "source": [
        "torch.save(model.state_dict(), \"gpt_tinyshakespeare.pth\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [],
      "source": [
        "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0004, weight_decay=0.1)\n",
        "\n",
        "torch.save({\n",
        "    \"model_state_dict\": model.state_dict(),\n",
        "    \"optimizer_state_dict\": optimizer.state_dict(),\n",
        "    }, \n",
        "    \"model_and_optimizer.pth\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [],
      "source": [
        "### How to load the model and optimizer states back:\n",
        "checkpoint = torch.load(\"model_and_optimizer.pth\")\n",
        "model = GPTModel(GPT_CONFIG_124M)\n",
        "model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-4, weight_decay=0.1)\n",
        "optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
        "model.train();"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "LOADING PRETRAINED WEIGHTS FROM OPENAI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate(model, idx, max_new_tokens, context_size, temperature=0.0, top_k=None, eos_id=None):\n",
        "\n",
        "    # For-loop is the same as before: Get logits, and only focus on last time step\n",
        "    for _ in range(max_new_tokens):\n",
        "        idx_cond = idx[:, -context_size:]\n",
        "        with torch.no_grad():\n",
        "            logits = model(idx_cond)\n",
        "        logits = logits[:, -1, :]\n",
        "\n",
        "        # New: Filter logits with top_k sampling\n",
        "        if top_k is not None:\n",
        "            # Keep only top_k values\n",
        "            top_logits, _ = torch.topk(logits, top_k)\n",
        "            min_val = top_logits[:, -1]\n",
        "            logits = torch.where(logits < min_val, torch.tensor(float(\"-inf\")).to(logits.device), logits)\n",
        "\n",
        "        # New: Apply temperature scaling\n",
        "        if temperature > 0.0:\n",
        "            logits = logits / temperature\n",
        "\n",
        "            # Apply softmax to get probabilities\n",
        "            probs = torch.softmax(logits, dim=-1)  # (batch_size, context_len)\n",
        "\n",
        "            # Sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)  # (batch_size, 1)\n",
        "\n",
        "        # Otherwise same as before: get idx of the vocab entry with the highest logits value\n",
        "        else:\n",
        "            idx_next = torch.argmax(logits, dim=-1, keepdim=True)  # (batch_size, 1)\n",
        "\n",
        "        if idx_next == eos_id:  # Stop generating early if end-of-sequence token is encountered and eos_id is specified\n",
        "            break\n",
        "\n",
        "        # Same as before: append sampled index to the running sequence\n",
        "        idx = torch.cat((idx, idx_next), dim=1)  # (batch_size, num_tokens+1)\n",
        "\n",
        "    return idx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING: You are using pip version 21.2.4; however, version 25.3 is available.\n",
            "You should consider upgrading via the 'C:\\Users\\j_san\\AppData\\Local\\Programs\\Python\\Python310\\python.exe -m pip install --upgrade pip' command.\n"
          ]
        }
      ],
      "source": [
        "!pip install tensorflow>=2.15.0 tqdm>=4.66"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TensorFlow version: 2.20.0\n",
            "tqdm version: 4.67.1\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import tqdm\n",
        "\n",
        "print(\"TensorFlow version:\", tf.__version__)\n",
        "print(\"tqdm version:\", tqdm.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import requests\n",
        "import json\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tqdm import tqdm\n",
        "\n",
        "def download_and_load_gpt2(model_size, models_dir):\n",
        "    # Valid model sizes: \"124M\", \"355M\", \"774M\", \"1558M\"\n",
        "    allowed_sizes = (\"124M\", \"355M\", \"774M\", \"1558M\")\n",
        "    if model_size not in allowed_sizes:\n",
        "        raise ValueError(f\"Model size not in {allowed_sizes}\")\n",
        "\n",
        "    model_dir = os.path.join(models_dir, model_size)\n",
        "    base_url = \"https://openaipublic.blob.core.windows.net/gpt-2/models\"\n",
        "    filenames = [\n",
        "        \"checkpoint\", \"encoder.json\", \"hparams.json\",\n",
        "        \"model.ckpt.data-00000-of-00001\", \"model.ckpt.index\",\n",
        "        \"model.ckpt.meta\", \"vocab.bpe\"\n",
        "    ]\n",
        "\n",
        "    os.makedirs(model_dir, exist_ok=True)\n",
        "\n",
        "    # 1. Download the files\n",
        "    for filename in filenames:\n",
        "        file_url = os.path.join(base_url, model_size, filename)\n",
        "        file_path = os.path.join(model_dir, filename)\n",
        "\n",
        "        if os.path.exists(file_path):\n",
        "            print(f\"{filename} already exists. Skipping download.\")\n",
        "            continue\n",
        "\n",
        "        print(f\"Downloading {filename}...\")\n",
        "        try:\n",
        "            r = requests.get(file_url, stream=True)\n",
        "            r.raise_for_status()\n",
        "            with open(file_path, \"wb\") as f:\n",
        "                file_size = int(r.headers.get(\"content-length\", 0))\n",
        "                chunk_size = 1000\n",
        "                with tqdm(total=file_size, unit=\"B\", unit_scale=True, desc=filename) as pbar:\n",
        "                    for chunk in r.iter_content(chunk_size=chunk_size):\n",
        "                        f.write(chunk)\n",
        "                        pbar.update(len(chunk))\n",
        "        except requests.exceptions.ConnectionError:\n",
        "            print(f\"Failed to download {filename} (Connection Error)\")\n",
        "\n",
        "    # 2. Load settings and params\n",
        "    tf_ckpt_path = tf.train.latest_checkpoint(model_dir)\n",
        "    settings = json.load(open(os.path.join(model_dir, \"hparams.json\")))\n",
        "    \n",
        "    # 3. Extract weights from TensorFlow checkpoint\n",
        "    params = {\"blocks\": [{} for _ in range(settings[\"n_layer\"])]}\n",
        "    \n",
        "    for name, _ in tf.train.list_variables(tf_ckpt_path):\n",
        "        variable_array = np.squeeze(tf.train.load_variable(tf_ckpt_path, name))\n",
        "        name_parts = name.split(\"/\")\n",
        "        \n",
        "        # Attention Blocks\n",
        "        if name_parts[0].startswith(\"h\"):\n",
        "            layer_number = int(name_parts[0][1:])\n",
        "            target_dict = params[\"blocks\"][layer_number]\n",
        "            sub_name = name_parts[1:]\n",
        "            \n",
        "            curr = target_dict\n",
        "            for key in sub_name[:-1]:\n",
        "                if key not in curr: curr[key] = {}\n",
        "                curr = curr[key]\n",
        "            curr[sub_name[-1]] = variable_array\n",
        "\n",
        "        # Other layers\n",
        "        elif name_parts[0] == \"wte\": params[\"wte\"] = variable_array\n",
        "        elif name_parts[0] == \"wpe\": params[\"wpe\"] = variable_array\n",
        "        elif name_parts[0] == \"ln_f\":\n",
        "            if name_parts[1] == \"g\": params[\"ln_f_g\"] = variable_array\n",
        "            elif name_parts[1] == \"b\": params[\"ln_f_b\"] = variable_array\n",
        "\n",
        "    return settings, params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Deleted corrupted 'gpt2' folder.\n"
          ]
        }
      ],
      "source": [
        "import shutil\n",
        "import os\n",
        "\n",
        "# Force delete the corrupted folder\n",
        "if os.path.exists(\"gpt2\"):\n",
        "    shutil.rmtree(\"gpt2\")\n",
        "    print(\"‚úÖ Deleted corrupted 'gpt2' folder.\")\n",
        "else:\n",
        "    print(\"Folder already gone.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading checkpoint...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "checkpoint: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 77.0/77.0 [00:00<?, ?B/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading encoder.json...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "encoder.json: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.04M/1.04M [00:03<00:00, 262kB/s] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading hparams.json...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "hparams.json: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 90.0/90.0 [00:00<?, ?B/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading model.ckpt.data-00000-of-00001...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "model.ckpt.data-00000-of-00001: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 498M/498M [30:44<00:00, 270kB/s]    \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading model.ckpt.index...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "model.ckpt.index: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5.21k/5.21k [00:00<00:00, 5.14MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading model.ckpt.meta...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "model.ckpt.meta: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 471k/471k [00:01<00:00, 255kB/s]  \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading vocab.bpe...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "vocab.bpe: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 456k/456k [00:01<00:00, 245kB/s]  \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Settings: {'n_vocab': 50257, 'n_ctx': 1024, 'n_embd': 768, 'n_head': 12, 'n_layer': 12}\n",
            "Parameter Keys: dict_keys(['blocks'])\n"
          ]
        }
      ],
      "source": [
        "# This will now work!\n",
        "settings, params = download_and_load_gpt2(model_size=\"124M\", models_dir=\"gpt2\")\n",
        "\n",
        "print(\"Settings:\", settings)\n",
        "print(\"Parameter Keys:\", params.keys())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Settings: {'n_vocab': 50257, 'n_ctx': 1024, 'n_embd': 768, 'n_head': 12, 'n_layer': 12}\n",
            "Parameter dictionary keys: dict_keys(['blocks'])\n"
          ]
        }
      ],
      "source": [
        "print(\"Settings:\", settings)\n",
        "print(\"Parameter dictionary keys:\", params.keys())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import requests\n",
        "import json\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tqdm import tqdm\n",
        "import shutil\n",
        "\n",
        "def download_and_load_gpt2(model_size, models_dir):\n",
        "    # Valid model sizes: \"124M\", \"355M\", \"774M\", \"1558M\"\n",
        "    allowed_sizes = (\"124M\", \"355M\", \"774M\", \"1558M\")\n",
        "    if model_size not in allowed_sizes:\n",
        "        raise ValueError(f\"Model size not in {allowed_sizes}\")\n",
        "\n",
        "    model_dir = os.path.join(models_dir, model_size)\n",
        "    base_url = \"https://openaipublic.blob.core.windows.net/gpt-2/models\"\n",
        "    filenames = [\n",
        "        \"checkpoint\", \"encoder.json\", \"hparams.json\",\n",
        "        \"model.ckpt.data-00000-of-00001\", \"model.ckpt.index\",\n",
        "        \"model.ckpt.meta\", \"vocab.bpe\"\n",
        "    ]\n",
        "\n",
        "    os.makedirs(model_dir, exist_ok=True)\n",
        "\n",
        "    # 1. Download the files\n",
        "    for filename in filenames:\n",
        "        file_url = os.path.join(base_url, model_size, filename)\n",
        "        file_path = os.path.join(model_dir, filename)\n",
        "\n",
        "        if os.path.exists(file_path):\n",
        "            print(f\"{filename} already exists. Skipping download.\")\n",
        "            continue\n",
        "\n",
        "        print(f\"Downloading {filename}...\")\n",
        "        try:\n",
        "            r = requests.get(file_url, stream=True)\n",
        "            r.raise_for_status()\n",
        "            with open(file_path, \"wb\") as f:\n",
        "                file_size = int(r.headers.get(\"content-length\", 0))\n",
        "                chunk_size = 1000\n",
        "                with tqdm(total=file_size, unit=\"B\", unit_scale=True, desc=filename) as pbar:\n",
        "                    for chunk in r.iter_content(chunk_size=chunk_size):\n",
        "                        f.write(chunk)\n",
        "                        pbar.update(len(chunk))\n",
        "        except requests.exceptions.ConnectionError:\n",
        "            print(f\"Failed to download {filename} (Connection Error)\")\n",
        "\n",
        "    # 2. Load settings and params\n",
        "    tf_ckpt_path = tf.train.latest_checkpoint(model_dir)\n",
        "    settings = json.load(open(os.path.join(model_dir, \"hparams.json\")))\n",
        "    \n",
        "    # 3. Extract weights from TensorFlow checkpoint\n",
        "    params = {\"blocks\": [{} for _ in range(settings[\"n_layer\"])]}\n",
        "    \n",
        "    for name, _ in tf.train.list_variables(tf_ckpt_path):\n",
        "        variable_array = np.squeeze(tf.train.load_variable(tf_ckpt_path, name))\n",
        "        \n",
        "        # FIX: Strip the \"model/\" prefix if it exists\n",
        "        name_parts = name.split(\"/\")\n",
        "        if name_parts[0] == \"model\":\n",
        "            name_parts = name_parts[1:] # Skip \"model\"\n",
        "        \n",
        "        # Attention Blocks\n",
        "        if name_parts[0].startswith(\"h\"):\n",
        "            layer_number = int(name_parts[0][1:])\n",
        "            target_dict = params[\"blocks\"][layer_number]\n",
        "            sub_name = name_parts[1:]\n",
        "            \n",
        "            curr = target_dict\n",
        "            for key in sub_name[:-1]:\n",
        "                if key not in curr: curr[key] = {}\n",
        "                curr = curr[key]\n",
        "            curr[sub_name[-1]] = variable_array\n",
        "\n",
        "        # Other layers\n",
        "        elif name_parts[0] == \"wte\": params[\"wte\"] = variable_array\n",
        "        elif name_parts[0] == \"wpe\": params[\"wpe\"] = variable_array\n",
        "        elif name_parts[0] == \"ln_f\":\n",
        "            if name_parts[1] == \"g\": params[\"ln_f_g\"] = variable_array\n",
        "            elif name_parts[1] == \"b\": params[\"ln_f_b\"] = variable_array\n",
        "\n",
        "    return settings, params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "checkpoint already exists. Skipping download.\n",
            "encoder.json already exists. Skipping download.\n",
            "hparams.json already exists. Skipping download.\n",
            "model.ckpt.data-00000-of-00001 already exists. Skipping download.\n",
            "model.ckpt.index already exists. Skipping download.\n",
            "model.ckpt.meta already exists. Skipping download.\n",
            "vocab.bpe already exists. Skipping download.\n",
            "Settings: {'n_vocab': 50257, 'n_ctx': 1024, 'n_embd': 768, 'n_head': 12, 'n_layer': 12}\n",
            "Parameter Keys: dict_keys(['blocks', 'ln_f_b', 'ln_f_g', 'wpe', 'wte'])\n"
          ]
        }
      ],
      "source": [
        "settings, params = download_and_load_gpt2(model_size=\"124M\", models_dir=\"gpt2\")\n",
        "\n",
        "print(\"Settings:\", settings)\n",
        "print(\"Parameter Keys:\", params.keys())\n",
        "# Should now print: dict_keys(['blocks', 'wte', 'wpe', 'ln_f_g', 'ln_f_b'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[-0.11010301 -0.03926672  0.03310751 ... -0.1363697   0.01506208\n",
            "   0.04531523]\n",
            " [ 0.04034033 -0.04861503  0.04624869 ...  0.08605453  0.00253983\n",
            "   0.04318958]\n",
            " [-0.12746179  0.04793796  0.18410145 ...  0.08991534 -0.12972379\n",
            "  -0.08785918]\n",
            " ...\n",
            " [-0.04453601 -0.05483596  0.01225674 ...  0.10435229  0.09783269\n",
            "  -0.06952604]\n",
            " [ 0.1860082   0.01665728  0.04611587 ... -0.09625227  0.07847701\n",
            "  -0.02245961]\n",
            " [ 0.05135201 -0.02768905  0.0499369  ...  0.00704835  0.15519823\n",
            "   0.12067825]]\n",
            "Token embedding weight tensor dimensions: (50257, 768)\n"
          ]
        }
      ],
      "source": [
        "print(params[\"wte\"])\n",
        "print(\"Token embedding weight tensor dimensions:\", params[\"wte\"].shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define model configurations in a dictionary for compactness\n",
        "model_configs = {\n",
        "    \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n",
        "    \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n",
        "    \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n",
        "    \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n",
        "}\n",
        "\n",
        "# Copy the base configuration and update with specific model settings\n",
        "model_name = \"gpt2-small (124M)\"  # Example model name\n",
        "NEW_CONFIG = GPT_CONFIG_124M.copy()\n",
        "NEW_CONFIG.update(model_configs[model_name])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {},
      "outputs": [],
      "source": [
        "NEW_CONFIG.update({\"context_length\": 1024, \"qkv_bias\": True})\n",
        "gpt = GPTModel(NEW_CONFIG)\n",
        "gpt.eval();"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {},
      "outputs": [],
      "source": [
        "def assign(left, right):\n",
        "    if left.shape != right.shape:\n",
        "        raise ValueError(f\"Shape mismatch. Left: {left.shape}, Right: {right.shape}\")\n",
        "    return torch.nn.Parameter(torch.tensor(right))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "def assign(left, right):\n",
        "    if left.shape != right.shape:\n",
        "        raise ValueError(f\"Shape mismatch. Left: {left.shape}, Right: {right.shape}\")\n",
        "    return torch.nn.Parameter(torch.tensor(right))\n",
        "\n",
        "def load_weights_into_gpt(gpt, params):\n",
        "    gpt.pos_emb.weight = assign(gpt.pos_emb.weight, params['wpe'])\n",
        "    gpt.tok_emb.weight = assign(gpt.tok_emb.weight, params['wte'])\n",
        "    \n",
        "    for b in range(len(params[\"blocks\"])):\n",
        "        # 1. Attention Weights\n",
        "        q_w, k_w, v_w = np.split(\n",
        "            (params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"w\"], 3, axis=-1)\n",
        "        gpt.trf_blocks[b].att.W_query.weight = assign(\n",
        "            gpt.trf_blocks[b].att.W_query.weight, q_w.T)\n",
        "        gpt.trf_blocks[b].att.W_key.weight = assign(\n",
        "            gpt.trf_blocks[b].att.W_key.weight, k_w.T)\n",
        "        gpt.trf_blocks[b].att.W_value.weight = assign(\n",
        "            gpt.trf_blocks[b].att.W_value.weight, v_w.T)\n",
        "\n",
        "        # 2. Attention Bias\n",
        "        q_b, k_b, v_b = np.split(\n",
        "            (params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"b\"], 3, axis=-1)\n",
        "        gpt.trf_blocks[b].att.W_query.bias = assign(\n",
        "            gpt.trf_blocks[b].att.W_query.bias, q_b)\n",
        "        gpt.trf_blocks[b].att.W_key.bias = assign(\n",
        "            gpt.trf_blocks[b].att.W_key.bias, k_b)\n",
        "        gpt.trf_blocks[b].att.W_value.bias = assign(\n",
        "            gpt.trf_blocks[b].att.W_value.bias, v_b)\n",
        "\n",
        "        # 3. Attention Projection\n",
        "        gpt.trf_blocks[b].att.out_proj.weight = assign(\n",
        "            gpt.trf_blocks[b].att.out_proj.weight, \n",
        "            params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"w\"].T)\n",
        "        gpt.trf_blocks[b].att.out_proj.bias = assign(\n",
        "            gpt.trf_blocks[b].att.out_proj.bias, \n",
        "            params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"b\"])\n",
        "\n",
        "        # 4. Feed Forward Weights\n",
        "        gpt.trf_blocks[b].ff.layers[0].weight = assign(\n",
        "            gpt.trf_blocks[b].ff.layers[0].weight, \n",
        "            params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"w\"].T)\n",
        "        gpt.trf_blocks[b].ff.layers[0].bias = assign(\n",
        "            gpt.trf_blocks[b].ff.layers[0].bias, \n",
        "            params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"b\"])\n",
        "        gpt.trf_blocks[b].ff.layers[2].weight = assign(\n",
        "            gpt.trf_blocks[b].ff.layers[2].weight, \n",
        "            params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"w\"].T)\n",
        "        gpt.trf_blocks[b].ff.layers[2].bias = assign(\n",
        "            gpt.trf_blocks[b].ff.layers[2].bias, \n",
        "            params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"b\"])\n",
        "\n",
        "        # 5. Layer Norms (Blocks)\n",
        "        gpt.trf_blocks[b].norm1.scale = assign(\n",
        "            gpt.trf_blocks[b].norm1.scale, \n",
        "            params[\"blocks\"][b][\"ln_1\"][\"g\"])\n",
        "        gpt.trf_blocks[b].norm1.shift = assign(\n",
        "            gpt.trf_blocks[b].norm1.shift, \n",
        "            params[\"blocks\"][b][\"ln_1\"][\"b\"])\n",
        "        gpt.trf_blocks[b].norm2.scale = assign(\n",
        "            gpt.trf_blocks[b].norm2.scale, \n",
        "            params[\"blocks\"][b][\"ln_2\"][\"g\"])\n",
        "        gpt.trf_blocks[b].norm2.shift = assign(\n",
        "            gpt.trf_blocks[b].norm2.shift, \n",
        "            params[\"blocks\"][b][\"ln_2\"][\"b\"])\n",
        "\n",
        "    # 6. Final Layer Norm (FIXED KEYS HERE)\n",
        "    gpt.final_norm.scale = assign(gpt.final_norm.scale, params[\"ln_f_g\"])\n",
        "    gpt.final_norm.shift = assign(gpt.final_norm.shift, params[\"ln_f_b\"])\n",
        "    \n",
        "    # 7. Output Head\n",
        "    gpt.out_head.weight = assign(gpt.out_head.weight, params[\"wte\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Weights successfully loaded!\n"
          ]
        }
      ],
      "source": [
        "load_weights_into_gpt(gpt, params)\n",
        "gpt.to(DEVICE)\n",
        "print(\"‚úÖ Weights successfully loaded!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Output text:\n",
            " Every effort moves you as far as the hand can go until the end of your turn unless something happens\n",
            "\n",
            "This would remove you from a battle\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(123)\n",
        "\n",
        "token_ids = generate(\n",
        "    model=gpt,\n",
        "    idx=text_to_token_ids(\"Every effort moves you\", tokenizer).to(DEVICE),\n",
        "    max_new_tokens=25,\n",
        "    context_size=NEW_CONFIG[\"context_length\"],\n",
        "    top_k=50,\n",
        "    temperature=1.5\n",
        ")\n",
        "\n",
        "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
