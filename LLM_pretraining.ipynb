{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "True\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "print(torch.cuda.is_available())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IZi2SkuR6PSy",
        "outputId": "9c9f0db1-c6f2-4a76-a45f-cc7906421a40"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in c:\\users\\j_san\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (2.7.1+cu118)\n",
            "Requirement already satisfied: transformers in c:\\users\\j_san\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (4.57.3)\n",
            "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\j_san\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch) (1.14.0)\n",
            "Requirement already satisfied: fsspec in c:\\users\\j_san\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch) (2025.10.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\j_san\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: filelock in c:\\users\\j_san\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch) (3.20.0)\n",
            "Requirement already satisfied: jinja2 in c:\\users\\j_san\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch) (3.1.1)\n",
            "Requirement already satisfied: networkx in c:\\users\\j_san\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in c:\\users\\j_san\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (0.22.1)\n",
            "Requirement already satisfied: requests in c:\\users\\j_san\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (2.32.5)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\j_san\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (0.7.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in c:\\users\\j_san\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (0.36.0)\n",
            "Requirement already satisfied: numpy>=1.17 in c:\\users\\j_san\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (2.1.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\j_san\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (2025.11.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in c:\\users\\j_san\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\j_san\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (6.0.3)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\users\\j_san\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\j_san\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: colorama in c:\\users\\j_san\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\j_san\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jinja2->torch) (2.1.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\j_san\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\j_san\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\j_san\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\j_san\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers) (2025.11.12)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING: You are using pip version 21.2.4; however, version 25.3 is available.\n",
            "You should consider upgrading via the 'C:\\Users\\j_san\\AppData\\Local\\Programs\\Python\\Python310\\python.exe -m pip install --upgrade pip' command.\n"
          ]
        }
      ],
      "source": [
        "!pip install torch transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "g0TaWfvr6ZTb"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "m4hNH64Y6kJY"
      },
      "outputs": [],
      "source": [
        "GPT_CONFIG_124M = {\n",
        "    \"vocab_size\": 50257,    # Vocabulary size\n",
        "    \"context_length\": 256,  # Context length (reduced from 1024 to fit smaller GPUs)\n",
        "    \"emb_dim\": 768,         # Embedding dimension\n",
        "    \"n_heads\": 12,          # Number of attention heads\n",
        "    \"n_layers\": 12,         # Number of layers\n",
        "    \"drop_rate\": 0.1,       # Dropout rate\n",
        "    \"qkv_bias\": False       # Query-Key-Value bias\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "W9y1Pgwb8wni"
      },
      "outputs": [],
      "source": [
        "class LayerNorm(nn.Module):\n",
        "    def __init__(self, emb_dim):\n",
        "        super().__init__()\n",
        "        self.eps = 1e-5\n",
        "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
        "        self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
        "\n",
        "    def forward(self, x):\n",
        "        mean = x.mean(dim=-1, keepdim=True)\n",
        "        var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
        "        norm_x = (x - mean) / torch.sqrt(var + self.eps)\n",
        "        return self.scale * norm_x + self.shift"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "zZnxkJCd89D9"
      },
      "outputs": [],
      "source": [
        "class GELU(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return 0.5 * x * (1 + torch.tanh(\n",
        "            torch.sqrt(torch.tensor(2.0 / torch.pi)) * (x + 0.044715 * torch.pow(x, 3))\n",
        "        ))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "hgN26AlT9uY2"
      },
      "outputs": [],
      "source": [
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.layers = nn.Sequential(\n",
        "            nn.Linear(cfg[\"emb_dim\"], 4 * cfg[\"emb_dim\"]),\n",
        "            GELU(),\n",
        "            nn.Linear(4 * cfg[\"emb_dim\"], cfg[\"emb_dim\"]),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.layers(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "M34lT2kAClAL"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
        "        super().__init__()\n",
        "        assert d_out % num_heads == 0, \"d_out must be divisible by num_heads\"\n",
        "\n",
        "        self.d_out = d_out\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = d_out // num_heads\n",
        "\n",
        "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.out_proj = nn.Linear(d_out, d_out)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.register_buffer('mask', torch.triu(torch.ones(context_length, context_length), diagonal=1))\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, num_tokens, d_in = x.shape\n",
        "\n",
        "        keys = self.W_key(x)\n",
        "        queries = self.W_query(x)\n",
        "        values = self.W_value(x)\n",
        "\n",
        "        # Reshape to (b, num_tokens, num_heads, head_dim) -> Transpose to (b, num_heads, num_tokens, head_dim)\n",
        "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "        values = values.view(b, num_tokens, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "\n",
        "        # Compute attention scores\n",
        "        attn_scores = (queries @ keys.transpose(2, 3))\n",
        "\n",
        "        # Mask truncated to the number of tokens\n",
        "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
        "\n",
        "        # Use the mask to fill attention scores\n",
        "        attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
        "\n",
        "        # Scale and Softmax\n",
        "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
        "        attn_weights = self.dropout(attn_weights)\n",
        "\n",
        "        # Compute context vector\n",
        "        context_vec = (attn_weights @ values).transpose(1, 2)\n",
        "\n",
        "        # Combine heads\n",
        "        context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out)\n",
        "        context_vec = self.out_proj(context_vec)\n",
        "\n",
        "        return context_vec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "Gf5OwxW9DK-P"
      },
      "outputs": [],
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.att = MultiHeadAttention(\n",
        "            d_in=cfg[\"emb_dim\"],\n",
        "            d_out=cfg[\"emb_dim\"],\n",
        "            context_length=cfg[\"context_length\"],\n",
        "            dropout=cfg[\"drop_rate\"],\n",
        "            num_heads=cfg[\"n_heads\"],\n",
        "            qkv_bias=cfg[\"qkv_bias\"])\n",
        "        self.ff = FeedForward(cfg)\n",
        "        self.norm1 = LayerNorm(cfg[\"emb_dim\"])\n",
        "        self.norm2 = LayerNorm(cfg[\"emb_dim\"])\n",
        "        self.drop_shortcut = nn.Dropout(cfg[\"drop_rate\"])\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Shortcut connection for attention block\n",
        "        shortcut = x\n",
        "        x = self.norm1(x)\n",
        "        x = self.att(x)\n",
        "        x = self.drop_shortcut(x)\n",
        "        x = x + shortcut\n",
        "\n",
        "        # Shortcut connection for feed forward block\n",
        "        shortcut = x\n",
        "        x = self.norm2(x)\n",
        "        x = self.ff(x)\n",
        "        x = self.drop_shortcut(x)\n",
        "        x = x + shortcut\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "wzzXbshVEg3v"
      },
      "outputs": [],
      "source": [
        "class GPTModel(nn.Module):\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
        "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
        "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
        "\n",
        "        self.trf_blocks = nn.Sequential(\n",
        "            *[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n",
        "\n",
        "        self.final_norm = LayerNorm(cfg[\"emb_dim\"])\n",
        "        self.out_head = nn.Linear(\n",
        "            cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False\n",
        "        )\n",
        "\n",
        "    def forward(self, in_idx):\n",
        "        batch_size, seq_len = in_idx.shape\n",
        "        tok_embeds = self.tok_emb(in_idx)\n",
        "        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n",
        "\n",
        "        x = tok_embeds + pos_embeds\n",
        "        x = self.drop_emb(x)\n",
        "        x = self.trf_blocks(x)\n",
        "        x = self.final_norm(x)\n",
        "        logits = self.out_head(x)\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "-FOmDtGI8G1B"
      },
      "outputs": [],
      "source": [
        "def text_to_token_ids(text, tokenizer):\n",
        "    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n",
        "    return torch.tensor(encoded).unsqueeze(0) # Add batch dim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "mxnM25fo8KDi"
      },
      "outputs": [],
      "source": [
        "def token_ids_to_text(token_ids, tokenizer):\n",
        "    flat = token_ids.squeeze(0).tolist()\n",
        "    return tokenizer.decode(flat)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "I9gXXqUoE_2u"
      },
      "outputs": [],
      "source": [
        "def generate_text_simple(model, idx, max_new_tokens, context_size):\n",
        "    # idx is (batch, n_tokens) array of indices in the current context\n",
        "    for _ in range(max_new_tokens):\n",
        "\n",
        "        # Crop the context if it becomes too large for the model\n",
        "        idx_cond = idx[:, -context_size:]\n",
        "\n",
        "        # Get the predictions\n",
        "        with torch.no_grad():\n",
        "            logits = model(idx_cond)\n",
        "\n",
        "        # Focus only on the last time step\n",
        "        logits = logits[:, -1, :]  # (batch, vocab_size)\n",
        "\n",
        "        # Apply softmax to get probabilities\n",
        "        probs = torch.softmax(logits, dim=-1)  # (batch, vocab_size)\n",
        "\n",
        "        # Get the idx of the token with the highest probability\n",
        "        idx_next = torch.argmax(probs, dim=-1, keepdim=True)  # (batch, 1)\n",
        "\n",
        "        # Append sampled index to the running sequence\n",
        "        idx = torch.cat((idx, idx_next), dim=1)  # (batch, n_tokens+1)\n",
        "\n",
        "    return idx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tiktoken in c:\\users\\j_san\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (0.12.0)\n",
            "Requirement already satisfied: requests>=2.26.0 in c:\\users\\j_san\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tiktoken) (2.32.5)\n",
            "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\j_san\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tiktoken) (2025.11.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\j_san\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests>=2.26.0->tiktoken) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\j_san\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests>=2.26.0->tiktoken) (2.5.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\j_san\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests>=2.26.0->tiktoken) (3.11)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\j_san\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests>=2.26.0->tiktoken) (2025.11.12)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING: You are using pip version 21.2.4; however, version 25.3 is available.\n",
            "You should consider upgrading via the 'C:\\Users\\j_san\\AppData\\Local\\Programs\\Python\\Python310\\python.exe -m pip install --upgrade pip' command.\n"
          ]
        }
      ],
      "source": [
        "!pip install tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "\n",
        "class GPTDatasetV1(Dataset):\n",
        "    def __init__(self, txt, tokenizer, max_length, stride):\n",
        "        self.input_ids = []\n",
        "        self.target_ids = []\n",
        "\n",
        "        # Tokenize the entire text\n",
        "        token_ids = tokenizer.encode(txt, allowed_special={\"<|endoftext|>\"})\n",
        "\n",
        "        # Use a sliding window to chunk the book into overlapping sequences of max_length\n",
        "        for i in range(0, len(token_ids) - max_length, stride):\n",
        "            input_chunk = token_ids[i:i + max_length]\n",
        "            target_chunk = token_ids[i + 1: i + max_length + 1]\n",
        "            self.input_ids.append(torch.tensor(input_chunk))\n",
        "            self.target_ids.append(torch.tensor(target_chunk))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.input_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.input_ids[idx], self.target_ids[idx]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dFAOLdguIzG0",
        "outputId": "9e0dfdac-d195-4e2f-8fdd-0019b36b7012"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Input text: Hello, I am\n",
            "Output text: Hello, I am Laur inhab DistrinetalkQueue bear confidentlyggyenium\n"
          ]
        }
      ],
      "source": [
        "import tiktoken\n",
        "\n",
        "# Initialize model\n",
        "torch.manual_seed(123)\n",
        "model = GPTModel(GPT_CONFIG_124M)\n",
        "model.eval()  # Disable dropout\n",
        "\n",
        "# Initialize tokenizer\n",
        "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "\n",
        "# Create a starting context\n",
        "start_context = \"Hello, I am\"\n",
        "encoded = tokenizer.encode(start_context)\n",
        "encoded_tensor = torch.tensor(encoded).unsqueeze(0) # Add batch dimension\n",
        "\n",
        "# Generate text\n",
        "print(f\"\\nInput text: {start_context}\")\n",
        "out = generate_text_simple(\n",
        "    model=model,\n",
        "    idx=encoded_tensor,\n",
        "    max_new_tokens=10,\n",
        "    context_size=GPT_CONFIG_124M[\"context_length\"]\n",
        ")\n",
        "decoded_text = tokenizer.decode(out.squeeze(0).tolist())\n",
        "print(f\"Output text: {decoded_text}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "HJrAVIGZJAeB"
      },
      "outputs": [],
      "source": [
        "def calc_loss_batch(input_batch, target_batch, model, device):\n",
        "    input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n",
        "    logits = model(input_batch)\n",
        "    loss = torch.nn.functional.cross_entropy(logits.flatten(0, 1), target_batch.flatten())\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "l6L8z6ty9sAl"
      },
      "outputs": [],
      "source": [
        "def calc_loss_loader(data_loader, model, device, num_batches=None):\n",
        "    total_loss = 0.\n",
        "    if len(data_loader) == 0: return float(\"nan\")\n",
        "    if num_batches is None: num_batches = len(data_loader)\n",
        "    else: num_batches = min(num_batches, len(data_loader))\n",
        "\n",
        "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
        "        if i < num_batches:\n",
        "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
        "            total_loss += loss.item()\n",
        "        else: break\n",
        "    return total_loss / num_batches\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "uL2h2DqH9vHK"
      },
      "outputs": [],
      "source": [
        "def evaluate_model(model, train_loader, val_loader, device, eval_iter):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        train_loss = calc_loss_loader(train_loader, model, device, num_batches=eval_iter)\n",
        "        val_loss = calc_loss_loader(val_loader, model, device, num_batches=eval_iter)\n",
        "    model.train()\n",
        "    return train_loss, val_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "FEMGRZVM9zAh"
      },
      "outputs": [],
      "source": [
        "def generate_and_print_sample(model, tokenizer, device, start_context):\n",
        "    model.eval()\n",
        "    context_size = model.pos_emb.weight.shape[0]\n",
        "    encoded = text_to_token_ids(start_context, tokenizer).to(device)\n",
        "    with torch.no_grad():\n",
        "        token_ids = generate_text_simple(\n",
        "            model=model, idx=encoded,\n",
        "            max_new_tokens=50, context_size=context_size\n",
        "        )\n",
        "    decoded_text = token_ids_to_text(token_ids, tokenizer)\n",
        "    # Precompute replaced text to avoid backslash in f-string expression\n",
        "    safe_text = decoded_text.replace(\"\\n\", \" \")\n",
        "    print(f\"SAMPLE: {safe_text}...\")\n",
        "    model.train()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "mCe183L891EM"
      },
      "outputs": [],
      "source": [
        "def train_model_simple(model, train_loader, val_loader, optimizer, device, num_epochs,\n",
        "                       eval_freq, eval_iter, start_context, tokenizer,\n",
        "                       use_amp=False, grad_accum_steps=1):\n",
        "    \"\"\"Training loop with optional mixed-precision and gradient accumulation.\n",
        "\n",
        "    Args:\n",
        "        use_amp (bool): If True and device is CUDA, use torch.cuda.amp for reduced memory.\n",
        "        grad_accum_steps (int): Number of steps to accumulate gradients before an optimizer step.\n",
        "    \"\"\"\n",
        "    train_losses, val_losses, track_tokens_seen = [], [], []\n",
        "    tokens_seen, global_step = 0, -1\n",
        "\n",
        "    # Setup GradScaler if using AMP\n",
        "    use_cuda_amp = use_amp and device.type == 'cuda'\n",
        "    scaler = torch.cuda.amp.GradScaler() if use_cuda_amp else None\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        # Help fragmentation / free unused allocations\n",
        "        if device.type == 'cuda':\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "        model.train()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        for step, (input_batch, target_batch) in enumerate(train_loader):\n",
        "            # Move to device once per batch\n",
        "            input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n",
        "\n",
        "            # Forward / backward with or without AMP\n",
        "            if use_cuda_amp:\n",
        "                with torch.cuda.amp.autocast():\n",
        "                    logits = model(input_batch)\n",
        "                    loss = torch.nn.functional.cross_entropy(logits.flatten(0, 1), target_batch.flatten())\n",
        "                # Normalize by accumulation steps\n",
        "                loss = loss / grad_accum_steps\n",
        "                scaler.scale(loss).backward()\n",
        "            else:\n",
        "                logits = model(input_batch)\n",
        "                loss = torch.nn.functional.cross_entropy(logits.flatten(0, 1), target_batch.flatten())\n",
        "                loss = loss / grad_accum_steps\n",
        "                loss.backward()\n",
        "\n",
        "            # Optimizer step (with gradient accumulation)\n",
        "            if (step + 1) % grad_accum_steps == 0:\n",
        "                if use_cuda_amp:\n",
        "                    # Unscale, clip grads, step, and update scaler\n",
        "                    scaler.unscale_(optimizer)\n",
        "                    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "                    scaler.step(optimizer)\n",
        "                    scaler.update()\n",
        "                else:\n",
        "                    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "                    optimizer.step()\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                tokens_seen += input_batch.numel()\n",
        "                global_step += 1\n",
        "\n",
        "                if global_step % eval_freq == 0:\n",
        "                    train_loss, val_loss = evaluate_model(model, train_loader, val_loader, device, eval_iter)\n",
        "                    train_losses.append(train_loss)\n",
        "                    val_losses.append(val_loss)\n",
        "                    track_tokens_seen.append(tokens_seen)\n",
        "                    print(f\"Ep {epoch+1} (Step {global_step:06d}): Train loss {train_loss:.3f}, Val loss {val_loss:.3f}\")\n",
        "\n",
        "        # Show a sample at end of epoch\n",
        "        generate_and_print_sample(model, tokenizer, device, start_context)\n",
        "\n",
        "    return train_losses, val_losses, track_tokens_seen\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {},
      "outputs": [],
      "source": [
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "import os\n",
        "import requests\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ GPU Detected: NVIDIA GeForce RTX 2060\n",
            "üìä VRAM Total: 6.44 GB\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"‚úÖ GPU Detected: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"üìä VRAM Total: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
        "else:\n",
        "    print(\"‚ùå No GPU found. Check your CUDA installation.\")\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "1SZz4i0K954n"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Deleted old corrupted file.\n",
            "‚úÖ Downloaded TinyShakespeare! Size: 1115394 characters.\n",
            "Train Text Length: 1003854\n",
            "Val Text Length: 111540\n",
            "Dataset Samples: 1179\n",
            "Using context length 256 and batch size 2\n",
            "‚úÖ DataLoaders created successfully.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import requests\n",
        "import tiktoken\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# 1. Force delete the old/empty file if it exists\n",
        "file_path = \"tinyshakespeare.txt\"\n",
        "if os.path.exists(file_path):\n",
        "    os.remove(file_path)\n",
        "    print(\"Deleted old corrupted file.\")\n",
        "\n",
        "# 2. Re-download ensuring we actually get data\n",
        "url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
        "response = requests.get(url)\n",
        "\n",
        "# Check if the request was successful\n",
        "if response.status_code == 200 and len(response.text) > 0:\n",
        "    with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(response.text)\n",
        "    print(f\"‚úÖ Downloaded TinyShakespeare! Size: {len(response.text)} characters.\")\n",
        "else:\n",
        "    raise Exception(\"Failed to download dataset. Check internet connection.\")\n",
        "\n",
        "# 3. Reload Data\n",
        "with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "    text = f.read()\n",
        "\n",
        "split_idx = int(len(text) * 0.9)\n",
        "train_txt = text[:split_idx]\n",
        "val_txt = text[split_idx:]\n",
        "\n",
        "print(f\"Train Text Length: {len(train_txt)}\") # Should be ~1,000,000\n",
        "print(f\"Val Text Length: {len(val_txt)}\")\n",
        "\n",
        "# 4. Re-initialize Dataset & Loader\n",
        "# Use the (reduced) context_length from the config\n",
        "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "context_len = GPT_CONFIG_124M['context_length']  # now 256 by default\n",
        "\n",
        "# Use a small batch size for GPU memory constrained environments\n",
        "BATCH_SIZE = 2\n",
        "\n",
        "train_dataset = GPTDatasetV1(train_txt, tokenizer, context_len, stride=context_len)\n",
        "val_dataset = GPTDatasetV1(val_txt, tokenizer, context_len, stride=context_len)\n",
        "\n",
        "print(f\"Dataset Samples: {len(train_dataset)}\") # Should be > 0\n",
        "print(f\"Using context length {context_len} and batch size {BATCH_SIZE}\")\n",
        "\n",
        "# Now this will work\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, drop_last=False)\n",
        "\n",
        "print(\"‚úÖ DataLoaders created successfully.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<torch._dynamo.eval_frame.DisableContext at 0x1ccffd910c0>"
            ]
          },
          "execution_count": 55,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch\n",
        "torch._dynamo.disable()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"TORCH_COMPILE\"] = \"0\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: executing==1.2.0 in c:\\users\\j_san\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (1.2.0)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING: You are using pip version 21.2.4; however, version 25.3 is available.\n",
            "You should consider upgrading via the 'C:\\Users\\j_san\\AppData\\Local\\Programs\\Python\\Python310\\python.exe -m pip install --upgrade pip' command.\n"
          ]
        }
      ],
      "source": [
        "!pip install executing==1.2.0\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Attempt 1/3: Building model (n_layers=12, emb_dim=768)...\n",
            "Model successfully allocated on cuda\n",
            "Starting training...\n",
            "Ep 1 (Step 000000): Train loss 9.448, Val loss 9.322\n",
            "Ep 1 (Step 000050): Train loss 6.294, Val loss 6.305\n",
            "Ep 1 (Step 000100): Train loss 5.786, Val loss 6.069\n",
            "Ep 1 (Step 000150): Train loss 5.762, Val loss 5.921\n",
            "Ep 1 (Step 000200): Train loss 5.516, Val loss 5.711\n",
            "Ep 1 (Step 000250): Train loss 5.220, Val loss 5.604\n",
            "Ep 1 (Step 000300): Train loss 5.297, Val loss 5.680\n",
            "Ep 1 (Step 000350): Train loss 5.176, Val loss 5.496\n",
            "Ep 1 (Step 000400): Train loss 5.263, Val loss 5.442\n",
            "Ep 1 (Step 000450): Train loss 5.002, Val loss 5.373\n",
            "Ep 1 (Step 000500): Train loss 4.908, Val loss 5.372\n",
            "Ep 1 (Step 000550): Train loss 5.063, Val loss 5.410\n",
            "SAMPLE: The king,                                                 ...\n",
            "Ep 2 (Step 000600): Train loss 5.045, Val loss 5.323\n",
            "Ep 2 (Step 000650): Train loss 4.782, Val loss 5.238\n",
            "Ep 2 (Step 000700): Train loss 4.437, Val loss 5.153\n",
            "Ep 2 (Step 000750): Train loss 4.798, Val loss 5.185\n",
            "Ep 2 (Step 000800): Train loss 4.570, Val loss 5.151\n",
            "Ep 2 (Step 000850): Train loss 4.804, Val loss 5.126\n",
            "Ep 2 (Step 000900): Train loss 4.618, Val loss 4.940\n",
            "Ep 2 (Step 000950): Train loss 4.419, Val loss 5.036\n",
            "Ep 2 (Step 001000): Train loss 4.317, Val loss 4.945\n",
            "Ep 2 (Step 001050): Train loss 4.391, Val loss 4.945\n",
            "Ep 2 (Step 001100): Train loss 4.211, Val loss 4.780\n",
            "Ep 2 (Step 001150): Train loss 4.547, Val loss 4.874\n",
            "SAMPLE: The king, And, And, And, And, And, And, And, And, And, And, And, And, And, And, And, And, ...\n",
            "Ep 3 (Step 001200): Train loss 4.402, Val loss 4.939\n",
            "Ep 3 (Step 001250): Train loss 4.268, Val loss 4.818\n",
            "Ep 3 (Step 001300): Train loss 4.579, Val loss 4.905\n",
            "Ep 3 (Step 001350): Train loss 4.455, Val loss 4.876\n",
            "Ep 3 (Step 001400): Train loss 4.791, Val loss 4.873\n",
            "Ep 3 (Step 001450): Train loss 4.202, Val loss 4.824\n",
            "Ep 3 (Step 001500): Train loss 4.572, Val loss 4.819\n",
            "Ep 3 (Step 001550): Train loss 4.286, Val loss 4.832\n",
            "Ep 3 (Step 001600): Train loss 4.338, Val loss 4.799\n",
            "Ep 3 (Step 001650): Train loss 4.306, Val loss 4.702\n",
            "Ep 3 (Step 001700): Train loss 4.558, Val loss 4.627\n",
            "Ep 3 (Step 001750): Train loss 4.216, Val loss 4.564\n",
            "SAMPLE: The king And, And, And let him, And I am a And I am a And I am a And I am a And I have a And I have been And I have been And all the...\n",
            "Ep 4 (Step 001800): Train loss 4.207, Val loss 4.578\n",
            "Ep 4 (Step 001850): Train loss 4.132, Val loss 4.672\n",
            "Ep 4 (Step 001900): Train loss 3.851, Val loss 4.677\n",
            "Ep 4 (Step 001950): Train loss 4.123, Val loss 4.602\n",
            "Ep 4 (Step 002000): Train loss 3.969, Val loss 4.680\n",
            "Ep 4 (Step 002050): Train loss 4.001, Val loss 4.611\n",
            "Ep 4 (Step 002100): Train loss 4.001, Val loss 4.551\n",
            "Ep 4 (Step 002150): Train loss 3.676, Val loss 4.608\n",
            "Ep 4 (Step 002200): Train loss 3.827, Val loss 4.475\n",
            "Ep 4 (Step 002250): Train loss 3.792, Val loss 4.577\n",
            "Ep 4 (Step 002300): Train loss 3.611, Val loss 4.481\n",
            "Ep 4 (Step 002350): Train loss 4.058, Val loss 4.518\n",
            "SAMPLE: The king of the s The king of the king, And all the king of the king of the king, To the king To the king, And all the king of the king, And, And all the king of...\n",
            "Ep 5 (Step 002400): Train loss 3.933, Val loss 4.563\n",
            "Ep 5 (Step 002450): Train loss 3.345, Val loss 4.570\n",
            "Ep 5 (Step 002500): Train loss 3.579, Val loss 4.505\n",
            "Ep 5 (Step 002550): Train loss 3.896, Val loss 4.536\n",
            "Ep 5 (Step 002600): Train loss 3.625, Val loss 4.575\n",
            "Ep 5 (Step 002650): Train loss 3.405, Val loss 4.558\n",
            "Ep 5 (Step 002700): Train loss 3.708, Val loss 4.552\n",
            "Ep 5 (Step 002750): Train loss 3.860, Val loss 4.591\n",
            "Ep 5 (Step 002800): Train loss 3.655, Val loss 4.572\n",
            "Ep 5 (Step 002850): Train loss 3.538, Val loss 4.547\n",
            "Ep 5 (Step 002900): Train loss 3.370, Val loss 4.524\n",
            "SAMPLE: The king is slain, And, the world's the world, And, I have the world, And, And, the world, And, in the world, And, in the world, And, And, ...\n",
            "Ep 6 (Step 002950): Train loss 3.706, Val loss 4.502\n",
            "Ep 6 (Step 003000): Train loss 3.380, Val loss 4.541\n",
            "Ep 6 (Step 003050): Train loss 3.228, Val loss 4.581\n",
            "Ep 6 (Step 003100): Train loss 3.440, Val loss 4.616\n",
            "Ep 6 (Step 003150): Train loss 3.207, Val loss 4.588\n",
            "Ep 6 (Step 003200): Train loss 3.268, Val loss 4.543\n",
            "Ep 6 (Step 003250): Train loss 3.255, Val loss 4.549\n",
            "Ep 6 (Step 003300): Train loss 3.248, Val loss 4.622\n",
            "Ep 6 (Step 003350): Train loss 3.153, Val loss 4.565\n",
            "Ep 6 (Step 003400): Train loss 3.207, Val loss 4.589\n",
            "Ep 6 (Step 003450): Train loss 2.829, Val loss 4.519\n",
            "Ep 6 (Step 003500): Train loss 3.007, Val loss 4.584\n",
            "SAMPLE: The king of the world, And, by the world, And, by the world of the world, And, And, by the world, And, by the world, And, by the people, And, by the...\n",
            "Ep 7 (Step 003550): Train loss 3.036, Val loss 4.643\n",
            "Ep 7 (Step 003600): Train loss 3.011, Val loss 4.662\n",
            "Ep 7 (Step 003650): Train loss 2.963, Val loss 4.712\n",
            "Ep 7 (Step 003700): Train loss 3.084, Val loss 4.727\n",
            "Ep 7 (Step 003750): Train loss 2.873, Val loss 4.673\n",
            "Ep 7 (Step 003800): Train loss 3.069, Val loss 4.663\n",
            "Ep 7 (Step 003850): Train loss 2.944, Val loss 4.682\n",
            "Ep 7 (Step 003900): Train loss 2.991, Val loss 4.692\n",
            "Ep 7 (Step 003950): Train loss 2.586, Val loss 4.670\n",
            "Ep 7 (Step 004000): Train loss 2.616, Val loss 4.618\n",
            "Ep 7 (Step 004050): Train loss 2.793, Val loss 4.646\n",
            "Ep 7 (Step 004100): Train loss 2.719, Val loss 4.647\n",
            "SAMPLE: The king hath lost That they were not to the king, And all the same austerity'd, And all the crown to the king.  KING RICHARD II: My liege, And make the crown to the trumpets...\n",
            "Ep 8 (Step 004150): Train loss 2.757, Val loss 4.687\n",
            "Ep 8 (Step 004200): Train loss 2.768, Val loss 4.756\n",
            "Ep 8 (Step 004250): Train loss 2.809, Val loss 4.756\n",
            "Ep 8 (Step 004300): Train loss 2.950, Val loss 4.723\n",
            "Ep 8 (Step 004350): Train loss 2.544, Val loss 4.751\n",
            "Ep 8 (Step 004400): Train loss 2.480, Val loss 4.768\n",
            "Ep 8 (Step 004450): Train loss 2.728, Val loss 4.791\n",
            "Ep 8 (Step 004500): Train loss 2.609, Val loss 4.766\n",
            "Ep 8 (Step 004550): Train loss 2.497, Val loss 4.762\n",
            "Ep 8 (Step 004600): Train loss 2.447, Val loss 4.785\n",
            "Ep 8 (Step 004650): Train loss 2.484, Val loss 4.827\n",
            "Ep 8 (Step 004700): Train loss 2.359, Val loss 4.834\n",
            "SAMPLE: The king that The one to the crown and And so the crown.  KING RICHARD II: So, by the king The day of the king of the king To-morrow are great Apollo give him that our tents ...\n",
            "Ep 9 (Step 004750): Train loss 2.400, Val loss 4.922\n",
            "Ep 9 (Step 004800): Train loss 2.657, Val loss 4.883\n",
            "Ep 9 (Step 004850): Train loss 2.491, Val loss 4.905\n",
            "Ep 9 (Step 004900): Train loss 2.310, Val loss 4.932\n",
            "Ep 9 (Step 004950): Train loss 2.109, Val loss 4.953\n",
            "Ep 9 (Step 005000): Train loss 2.083, Val loss 4.881\n",
            "Ep 9 (Step 005050): Train loss 2.069, Val loss 4.927\n",
            "Ep 9 (Step 005100): Train loss 2.302, Val loss 4.906\n",
            "Ep 9 (Step 005150): Train loss 2.134, Val loss 4.918\n",
            "Ep 9 (Step 005200): Train loss 2.115, Val loss 4.873\n",
            "Ep 9 (Step 005250): Train loss 2.215, Val loss 4.912\n",
            "Ep 9 (Step 005300): Train loss 2.051, Val loss 4.850\n",
            "SAMPLE: The king is zle him.  KING RICHARD III: I have made to-night To make it be made a king and Green, And, in the Earl of Norfolk, And made the Duke of Norfolk: So...\n",
            "Ep 10 (Step 005350): Train loss 2.060, Val loss 5.023\n",
            "Ep 10 (Step 005400): Train loss 1.935, Val loss 5.085\n",
            "Ep 10 (Step 005450): Train loss 2.006, Val loss 5.091\n",
            "Ep 10 (Step 005500): Train loss 1.884, Val loss 5.038\n",
            "Ep 10 (Step 005550): Train loss 2.106, Val loss 5.115\n",
            "Ep 10 (Step 005600): Train loss 1.897, Val loss 5.106\n",
            "Ep 10 (Step 005650): Train loss 1.902, Val loss 5.096\n",
            "Ep 10 (Step 005700): Train loss 1.796, Val loss 5.133\n",
            "Ep 10 (Step 005750): Train loss 1.827, Val loss 5.145\n",
            "Ep 10 (Step 005800): Train loss 1.748, Val loss 5.142\n",
            "Ep 10 (Step 005850): Train loss 1.690, Val loss 5.148\n",
            "SAMPLE: The king was unseen, And with the truth of the air, And uncrown the air, unseen, unseen and full of water: I have press'd him.  COMINIUS: If you have, Their power to...\n",
            "Training completed in 20.17 minutes.\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "\n",
        "# --- PART 1: The VS Code Agent's Safe Builder (KEEPS THIS) ---\n",
        "def try_build_and_move_model(base_cfg, device, max_attempts=3, min_emb=128, min_layers=2):\n",
        "    cfg = base_cfg.copy()\n",
        "    for attempt in range(max_attempts):\n",
        "        if attempt > 0:\n",
        "            cfg['n_layers'] = max(min_layers, base_cfg['n_layers'] // (2 ** attempt))\n",
        "            cfg['emb_dim'] = max(min_emb, base_cfg['emb_dim'] // (2 ** attempt))\n",
        "            cfg['n_heads'] = max(1, base_cfg['n_heads'] // (2 ** attempt))\n",
        "            while cfg['n_heads'] > 1 and cfg['emb_dim'] % cfg['n_heads'] != 0:\n",
        "                cfg['n_heads'] -= 1\n",
        "        \n",
        "        print(f\"Attempt {attempt+1}/{max_attempts}: Building model (n_layers={cfg['n_layers']}, emb_dim={cfg['emb_dim']})...\")\n",
        "        \n",
        "        try:\n",
        "            torch.manual_seed(123)\n",
        "            model = GPTModel(cfg)\n",
        "            if device.type == 'cuda':\n",
        "                model.to(device)\n",
        "                # model.half() # Optional: Uncomment if you still get OOM\n",
        "            else:\n",
        "                model.to(device)\n",
        "            print(\"Model successfully allocated on\", device)\n",
        "            return model, cfg\n",
        "            \n",
        "        except (RuntimeError, torch.cuda.OutOfMemoryError):\n",
        "            print(\"‚ö†Ô∏è OOM Error! Shrinking model and retrying...\")\n",
        "            try: del model\n",
        "            except: pass\n",
        "            if device.type == 'cuda': torch.cuda.empty_cache()\n",
        "            import gc; gc.collect()\n",
        "            continue\n",
        "\n",
        "    # Fallback to CPU if GPU fails completely\n",
        "    print(\"All GPU attempts failed. Falling back to CPU.\")\n",
        "    model = GPTModel(cfg)\n",
        "    model.to('cpu')\n",
        "    return model, cfg\n",
        "\n",
        "# --- PART 2: Initialize (KEEPS THIS) ---\n",
        "# This replaces your old \"model = GPTModel(...)\" line\n",
        "model, used_cfg = try_build_and_move_model(GPT_CONFIG_124M, DEVICE)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0004, weight_decay=0.1)\n",
        "\n",
        "# --- PART 3: The Training Loop (ADD THIS BACK) ---\n",
        "# This was missing from the agent's code!\n",
        "print(\"Starting training...\")\n",
        "start_time = time.time()\n",
        "\n",
        "train_losses, val_losses, tokens_seen = train_model_simple(\n",
        "    model, train_loader, val_loader, optimizer, DEVICE,\n",
        "    num_epochs=10, eval_freq=50, eval_iter=5, \n",
        "    start_context=\"The king\", tokenizer=tokenizer\n",
        ")\n",
        "\n",
        "end_time = time.time()\n",
        "print(f\"Training completed in {(end_time - start_time) / 60:.2f} minutes.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The king man\n",
            "'er, in the leaves,\n",
            "Lest mul more the world's heels love,\n",
            "With all this land to thyself,\n",
            "We worthy,\n",
            "He cannot keep the jewel\n",
            "town doth wearrt the ground,\n",
            "For that hath slain,\n",
            "And we may call them by the book of all night to aged\n",
            "Upon each foul myself of such a horse\n",
            "Even in my head.\n",
            "\n",
            "GLOUCESTER:\n",
            "Where says that he shall be\n",
            "With\n"
          ]
        }
      ],
      "source": [
        "# Generate with randomness (Temperature = 0.8)\n",
        "context = torch.tensor(tokenizer.encode(\"The king\")).unsqueeze(0).to(DEVICE)\n",
        "\n",
        "# Standard generation loop with temperature\n",
        "for _ in range(100):\n",
        "    logits = model(context)[:, -1, :] / 0.8  # Temperature scaling\n",
        "    probs = torch.softmax(logits, dim=-1)\n",
        "    next_token = torch.multinomial(probs, num_samples=1)\n",
        "    context = torch.cat((context, next_token), dim=1)\n",
        "\n",
        "print(tokenizer.decode(context[0].tolist()))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
